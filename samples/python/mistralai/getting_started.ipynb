{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with GitHub Models - Mistral AI SDK\n",
    "\n",
    "## 1. Personal access token\n",
    "\n",
    "A personal access token is made available in the Codespaces environment in the `GITHUB_TOKEN` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mistralai --quiet\n",
    "%pip install python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Set environment variables and create the client\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    raise ValueError(\"GITHUB_TOKEN is not set\")\n",
    "\n",
    "github_token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "# Pick one of the Mistral models from the GitHub Models service\n",
    "model_name = \"Mistral-large\"\n",
    "\n",
    "client = MistralClient(api_key=github_token, endpoint=endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run a basic code sample\n",
    "\n",
    "This is just calling the `chat.completions` endpoint with a simple prompt.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        }\n",
    "    ],\n",
    "    model=model_name,\n",
    "    # Optional parameters\n",
    "    temperature=1.,\n",
    "    max_tokens=1000,\n",
    "    top_p=1.    \n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Turn Conversation\n",
    "\n",
    "This sample demonstrates a multi-turn conversation with the chat completion API.\n",
    "When using the model for a chat application, you'll need to manage the history of that\n",
    "conversation and send the latest messages to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the chat completion API\n",
    "response = client.chat(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"The capital of France is Paris.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What about Spain?\",\n",
    "        },\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Streaming the response\n",
    "\n",
    "For a better user experience, you will want to stream the response of the model\n",
    "so that the first token shows up early and you avoid waiting for long responses.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the chat completion API\n",
    "response = client.chat_stream(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me 5 good reasons why I should exercise every day.\",\n",
    "        },\n",
    "    ],\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "# Print the streamed response\n",
    "for update in response:\n",
    "    if update.choices[0].delta.content:\n",
    "        print(update.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tools and Function Calling\n",
    "\n",
    "A language model like `mistral-large` can be given a set of tools it can ask the calling program to invoke,\n",
    "for running specific actions depending on the context of the conversation.\n",
    "This sample demonstrates how to define a function tool and how to act on a request from the model to invoke it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a function that returns flight information between two cities (mock implementation)\n",
    "def get_flight_info(origin_city: str, destination_city: str):\n",
    "    if origin_city == \"Seattle\" and destination_city == \"Miami\":\n",
    "        return json.dumps(\n",
    "            {\n",
    "                \"airline\": \"Delta\",\n",
    "                \"flight_number\": \"DL123\",\n",
    "                \"flight_date\": \"May 7th, 2024\",\n",
    "                \"flight_time\": \"10:00AM\",\n",
    "            }\n",
    "        )\n",
    "    return json.dump({\"error\": \"No flights found between the cities\"})\n",
    "\n",
    "\n",
    "# Define a function tool that the model can ask to invoke in order to retrieve flight information\n",
    "tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_flight_info\",\n",
    "        \"description\": \"\"\"Returns information about the next flight between two cities.\n",
    "            This includes the name of the airline, flight number and the date and time\n",
    "            of the next flight\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"origin_city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the city where the flight originates\",\n",
    "                },\n",
    "                \"destination_city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The flight destination city\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"origin_city\", \"destination_city\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You an assistant that helps users find flight information.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm interested in going to Miami. What is the next flight there from Seattle?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "response = client.chat(\n",
    "    messages=messages,\n",
    "    tools=[tool],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "# We expect the model to ask for a tool call\n",
    "if response.choices[0].finish_reason == \"tool_calls\":\n",
    "\n",
    "    # Append the model response to the chat history\n",
    "    messages.append(response.choices[0].message)\n",
    "\n",
    "    # We expect a single tool call\n",
    "    if (\n",
    "        response.choices[0].message.tool_calls\n",
    "        and len(response.choices[0].message.tool_calls) == 1\n",
    "    ):\n",
    "\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "\n",
    "        # We expect the tool to be a function call\n",
    "        if tool_call.type == \"function\":\n",
    "\n",
    "            # Parse the function call arguments and call the function\n",
    "            function_args = json.loads(tool_call.function.arguments.replace(\"'\", '\"'))\n",
    "            print(\n",
    "                f\"Calling function `{tool_call.function.name}` with arguments {function_args}\"\n",
    "            )\n",
    "            callable_func = locals()[tool_call.function.name]\n",
    "            function_return = callable_func(**function_args)\n",
    "            print(f\"Function returned = {function_return}\")\n",
    "\n",
    "            # Append the function call result fo the chat history\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": tool_call.function.name,\n",
    "                    \"content\": function_return,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Get another response from the model\n",
    "            response = client.chat(\n",
    "                messages=messages,\n",
    "                tools=[tool],\n",
    "                model=model_name,\n",
    "            )\n",
    "\n",
    "            print(f\"Model response = {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To learn more about what you can do with the Mistal models on GitHub using Python, check out the following cookbooks:\n",
    "\n",
    "- [How to call functions with chat models](../../../cookbooks/python/mistralai/function_calling.ipynb): This notebook goes into detail on how to get Mistral models to determine which of a set of functions to call to answer a user's question.\n",
    "- [Evaluation](../../../cookbooks/python/mistralai/evaluation.ipynb):\n",
    "This Jupyter Notebook demonstrates how to use LLMs to evaluate the responsed of LLMs.\n",
    "- [Prompting Capabilites](../../../cookbooks/python/mistralai/prompting_capabilities.ipynb): Shows how to use prompting capabilities to improve the performance of the model.\n",
    "\n",
    "\n",
    "Or check out [this folder for more cookbooks](../../../cookbooks/python/README.md).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gh-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
