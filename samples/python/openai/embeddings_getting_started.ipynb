{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with GitHub Models - OpenAI SDK and embeddings\n",
    "\n",
    "## 1. Personal access token\n",
    "\n",
    "A personal access token is made available in the Codespaces environment in the `GITHUB_TOKEN` environment variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai --quiet\n",
    "%pip install python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Set environment variables and create the client\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    raise ValueError(\"GITHUB_TOKEN is not set\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GITHUB_TOKEN\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://models.inference.ai.azure.com/\"\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embed a string\n",
    "\n",
    "This is just calling the `embeddings.create` endpoint with a simple prompt.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"text-embedding-3-small\" \n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=model_name,\n",
    "    input=[\"Hello, world!\"]\n",
    ")\n",
    "embeddings = response.data[0].embedding\n",
    "print(len(embeddings))\n",
    "print(embeddings[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response from the `text-embedding-3-small` model contains a vector of length 1536, which is the embedding of the input string. Different models have different embedding sizes. Please consult the [model documentation](https://github.com/marketplace/models/) for more information about the embedding model you are using. \n",
    "\n",
    "## 5. Embed a list of strings\n",
    "\n",
    "To save on API calls, you can embed a list of strings in a single call. The response will contain a list of embeddings, one for each input string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"text-embedding-3-small\" \n",
    "inputs = [\"Hello, world!\", \"How are you?\", \"What's the weather like?\"]\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=model_name,\n",
    "    input=inputs\n",
    ")\n",
    "for data in response.data:\n",
    "    print(data.embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "A common use case for embeddings in generative AI is to use them to implement Retrieval Augmented Generation (RAG) systems.\n",
    "\n",
    "See the cookbook [rag_getting_started](../../../cookbooks/python/llamaindex/rag_getting_started.ipynb) for an example of how to do this using the LLamaIndex framework.\n",
    "\n",
    "To learn more about what you can do with the GitHub models using the OpenAI Python API, [check out theses cookbooks](../../../cookbooks/python/openai/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gh-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
