{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GPT-4o with Images and Videos\n",
    "\n",
    "\n",
    "GPT-4o is a large multimodal model (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. It incorporates both natural language processing and visual understanding.\n",
    "\n",
    "The GPT-4o model answers general questions about what's present in the images. You can also show it video if you break the video into individual frames.\n",
    "\n",
    "To run this notebook in Codespaces, you need to run the following command on the terminal to install opencv and the underlying libraries. If you are running this notebook locally, your installation steps may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update; sudo apt-get install libgl1 -y\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Images\n",
    "\n",
    "The following command shows the most basic way to use the GPT-4o model with code. If this is your first time using these models programmatically, we recommend starting with our [OpenAI getting started notebook](../../../samples/python/openai/getting_started.ipynb). \n",
    "\n",
    "We will start by creating a client object based on the envionment variable `OPENAI_API_KEY` that you have set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time  # for measuring time duration of API calls\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    raise ValueError(\"GITHUB_TOKEN is not set\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GITHUB_TOKEN\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://models.github.ai/inference\"\n",
    "\n",
    "GPT_MODEL = \"openai/gpt-4o-mini\"\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call the client's chat completions **create** method. The following code shows a sample request body. The format is the same as is used for other chat completions with GPT-4o, except that the message content contains array with text and images (either a valid HTTP or HTTPS URL to an image, or a base-64-encoded image). \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_image(image_url):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "            { \"role\": \"user\", \"content\": [  \n",
    "                { \n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Describe this picture:\" \n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url\n",
    "                    }\n",
    "                }\n",
    "            ] } \n",
    "        ],\n",
    "        max_tokens=2000 \n",
    "    )\n",
    "    return response\n",
    "\n",
    "image_url = \"https://github.com/microsoft/assistant-pf-demo/blob/main/images/sad-puppy.png?raw=true\"\n",
    "\n",
    "print(describe_image(image_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a local image\n",
    "\n",
    "If you want to use a local image, you can use the following Python code to convert it to base64 so it can be passed to the API. Alternative file conversion tools are available online. Let's try ask gpt-4o-mini about the image below.\n",
    "\n",
    "![](data/sad-puppy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64, json\n",
    "from mimetypes import guess_type\n",
    "\n",
    "# Function to encode a local image into data URL \n",
    "def local_image_to_data_url(image_path):\n",
    "    # Guess the MIME type of the image based on the file extension\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'  # Default MIME type if none is found\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n",
    "\n",
    "# Example usage\n",
    "image_path = 'data/sad-puppy.png'\n",
    "data_url = local_image_to_data_url(image_path)\n",
    "print(f\"Data URL: {data_url[:100]}...\")\n",
    "response = describe_image(data_url)\n",
    "print(json.dumps(response.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The API response should look something like the following.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"chatcmpl-9vT1HlFN3bzRXTbYQgtRwPLfnGMxk\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 0,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"The picture features a fluffy, light-colored puppy sitting on a rug. In front of the puppy is a dark-colored food bowl. The setting appears warm and cozy, with wooden furniture and a neutral-colored wall in the background. The overall mood conveys a sense of comfort and playfulness.\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"function_call\": null,\n",
    "        \"tool_calls\": null\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"created\": 1723483275,\n",
    "  \"model\": \"gpt-4o-mini\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"service_tier\": null,\n",
    "  \"system_fingerprint\": \"fp_276aa25277\",\n",
    "  \"usage\": {\n",
    "    \"completion_tokens\": 57,\n",
    "    \"prompt_tokens\": 262,\n",
    "    \"total_tokens\": 319\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Every response includes a `\"finish_details\"` field. It has the following possible values:\n",
    "- `stop`: API returned complete model output.\n",
    "- `length`: Incomplete model output due to the `max_tokens` input parameter or model's token limit.\n",
    "- `content_filter`: Omitted content due to a flag from our content filters.\n",
    "\n",
    "## Detail parameter settings in image processing: Low, High, Auto  \n",
    "\n",
    "The _detail_ parameter in the model offers three choices: `low`, `high`, or `auto`, to adjust the way the model interprets and processes images. The default setting is auto, where the model decides between low or high based on the size of the image input. \n",
    "- `low` setting: the model does not activate the \"high res\" mode, instead processes a lower resolution 512x512 version, resulting in quicker responses and reduced token consumption for scenarios where fine detail isn't crucial.\n",
    "- `high` setting: the model activates \"high res\" mode. Here, the model initially views the low-resolution image and then generates detailed 512x512 segments from the input image. Each segment uses double the token budget, allowing for a more detailed interpretation of the image.''\n",
    "\n",
    "For details on how the image parameters impact tokens used please see [Image Tokens]() below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def describe_image_detail(image_url, detail = \"auto\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "            { \"role\": \"user\", \"content\": [  \n",
    "                { \n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Describe this picture:\" \n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"detail\": detail,\n",
    "                        \"url\": image_url\n",
    "                    }\n",
    "                }\n",
    "            ] } \n",
    "        ],\n",
    "        max_tokens=2000 \n",
    "    )\n",
    "    return response\n",
    "\n",
    "for detail in [\"auto\", \"low\", \"high\"]:\n",
    "    response = describe_image_detail(data_url, detail=detail)\n",
    "    print(detail, response.usage.prompt_tokens, response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Videos\n",
    "\n",
    "To process videos, you need to break them down into individual frames and then pass the sequence of frames to the model. To save on tokens, you can skip frames or only process a subset of the frames. It is also advisable to use `low` detail setting for videos to save on tokens and be able to process more frames.\n",
    "\n",
    "Below we are processing the well-known Big Buck Bunny video\n",
    "\n",
    "[![Watch the video](https://img.youtube.com/vi/aqz-KE-bpKQ/hqdefault.jpg)](https://www.youtube.com/watch?v=aqz-KE-bpKQ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's start by reading the frames into memory as base64 encoded images. We will then pass the frames to the model for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Audio\n",
    "import cv2\n",
    "\n",
    "video = cv2.VideoCapture(\"https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4\")\n",
    "\n",
    "# Get the frame rate of the video\n",
    "frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Frame Rate: {frame_rate} FPS\")\n",
    "\n",
    "\n",
    "base64Frames = []\n",
    "while video.isOpened():\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        break\n",
    "    _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "video.release()\n",
    "print(len(base64Frames), \"frames read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play the first 10s of the video\n",
    "import time\n",
    "display_handle = display(None, display_id=True)\n",
    "for img in base64Frames[:int(frame_rate * 10)]:\n",
    "    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\"))))\n",
    "    time.sleep(1/frame_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the video frames, we craft our prompt and send a request to GPT:\n",
    "> Note that we don't need to send every frame for GPT to understand what's going on in the video. In this case, we will send one frame per second.\n",
    "\n",
    "> Also, the current limit of the number of images that can be sent in one request is 250. If you have more than 250 frames, you will need to split them into multiple requests. Additional limits exist for the free tier of GitHub Models API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will process the video in batches and have the model describe the whole plot of the video. Due to the token limits of the free service, you need to process the video in rather small batches (we are using 50 here). And, we only sample one frame every 4 seconds. You can play with those parameters to get the best results for your video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4 # seconds\n",
    "one_frame_per_n_seconds = base64Frames[0::int(n*frame_rate)]\n",
    "print(len(one_frame_per_n_seconds), \"frames to describe.\")\n",
    "# process the whole video in 50 frames intervals\n",
    "batch_size = 50\n",
    "plot = \"\"\n",
    "for i in range(0, len(one_frame_per_n_seconds), batch_size):\n",
    "    print(f\"Processing frames {i} to {i+batch_size}\")\n",
    "    frame_batch = one_frame_per_n_seconds[i:i+batch_size]\n",
    "    PROMPT_MESSAGES = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are being presented with video frames and are asked to generate the plot of the video from them. \"\n",
    "                f\"You will be shown the video's plot up to and then {batch_size} frames, representing the next {batch_size} seconds of the video. \"\n",
    "                \"Expand on the plot of the video based on these frames. \"\n",
    "                f\"The plot so far is: \\n {plot}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *map(lambda image_url: { \n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"detail\": \"low\",\n",
    "                            \"url\": f\"data:image/jpg;base64,{image_url}\"\n",
    "                        }\n",
    "                    }, frame_batch),\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    result = client.chat.completions.create(model=\"openai/gpt-4o-mini\", messages=PROMPT_MESSAGES)\n",
    "    print(f\"used {result.usage.prompt_tokens} prompt tokens and {result.usage.completion_tokens} completion tokens.\\n\")\n",
    "    print(result.choices[0].message.content)\n",
    "    plot += result.choices[0].message.content + \"\\n\"   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image tokens \n",
    "\n",
    "The token cost of an input image depends on two main factors: the size of the image and the detail setting (low or high) used for each image. Here's a breakdown of how it works:\n",
    "\n",
    "- **Detail: Low resolution mode**\n",
    "    - Low detail allows the API to return faster responses and consume fewer input tokens for use cases that don’t require high detail.\n",
    "    - These images cost 85 tokens each, regardless of the image size.\n",
    "    - **Example: 4096 x 8192 image (low detail)**: The cost is a fixed 85 tokens, because it's a low detail image, and the size doesn't affect the cost in this mode.\n",
    "      \n",
    "- **Detail: High resolution mode**\n",
    "    - High detail lets the API see the image in more detail by cropping it into smaller squares. Each square uses more tokens to generate text.\n",
    "    - The token cost is calculated by a series of scaling steps:\n",
    "        1. The image is first scaled to fit within a 2048 x 2048 square while maintaining its aspect ratio.\n",
    "        1. The image is then scaled down so that the shortest side is 768 pixels long.\n",
    "        1. The image is divided into 512-pixel square tiles, and the number of these tiles (rounding up for partial tiles) determines the final cost. Each tile costs 170 tokens.\n",
    "        1. An additional 85 tokens are added to the total cost.\n",
    "    - **Example: 2048 x 4096 image (high detail)**\n",
    "        1. Initially resized to 1024 x 2048 to fit in the 2048 square.\n",
    "        1. Further resized to 768 x 1536.\n",
    "        1. Requires six 512px tiles to cover.\n",
    "        1. Total cost is `170 × 6 + 85 = 1105` tokens.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
